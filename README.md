# ğŸ§  Multi-Source Q&A Chatbot with LangChain + TinyLlama + FAISS

![LangChain](https://img.shields.io/badge/LangChain-ğŸ¦œ-blue?style=flat-square)
![Offline Capable](https://img.shields.io/badge/Offline-Ready-success?style=flat-square)
![Python](https://img.shields.io/badge/Python-3.9+-yellow?style=flat-square)
![License](https://img.shields.io/github/license/yourusername/langchain-chatbot?style=flat-square)

> Ask intelligent questions across multiple sources â€” all powered **locally** with Hugging Face, FAISS, and LangChain.

---

## âœ¨ Overview

A fully offline, multi-source chatbot powered by local LLMs and semantic search, built using:

- ğŸ¦™ **TinyLlama-1.1B-Chat** for question answering
- ğŸ§  **MiniLM** for fast and efficient embeddings
- ğŸ” **FAISS** for chunk retrieval
- ğŸ›  **LangChain** to orchestrate everything
- ğŸ¨ **Streamlit** for an elegant UI

---

## ğŸ“ Project Structure

```
LANGCHAIN/
â”‚
â”œâ”€â”€ chatbot/
â”‚   â”œâ”€â”€ models/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ model_downloader.py
â”œâ”€â”€ temp.pdf
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”‚
â”œâ”€â”€ venv/
â”‚   â”œâ”€â”€ etc/
â”‚   â”œâ”€â”€ Include/
â”‚   â”œâ”€â”€ Lib/
â”‚   â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ share/
â”‚   â””â”€â”€ pyvenv.cfg
â”œâ”€â”€ requirements.txt
```



---

## ğŸ“š Supported Sources

| Source     | Description                          |
|------------|--------------------------------------|
| ğŸ“š Wikipedia | Search by topic and query answers    |
| ğŸ“„ PDF       | Upload PDF files to extract insights |
| ğŸ“œ arXiv     | Search and analyze research papers   |
| ğŸŒ Website   | Scrape and query content dynamically |

---

## âš™ï¸ Setup Instructions

### 1. Create a Virtual Environment

```bash
cd Langchain
python -m venv venv
venv\Scripts\activate        # Windows
# OR
source venv/bin/activate     # macOS/Linux
```
### 2.Install Required Packages

```bash
pip install -r chatbot/requirements.txt
```
### 3.Download the Required Models

```bash
python chatbot/model_downloader.py
```
# Downloads and caches:

TinyLlama/TinyLlama-1.1B-Chat-v1.0

sentence-transformers/all-MiniLM-L6-v2

### 4.Run the App

```bash
streamlit run chatbot/app.py
```

### 5.env File

LANGCHAIN_TRACING_V2=false

LANGCHAIN_API_KEY=your_optional_key

LANGSMITH_ENDPOINT="https://api.smith.langchain.com"

LANGSMITH_PROJECT="multisource-chatbot"

USER_AGENT="LangChainBot/1.0"

## ğŸ§­ Architecture Overview

<details>
<summary>ğŸ§  Click to view architecture (Mermaid)</summary>

<br>


```mermaid
graph TD
    A[User Input via Streamlit] --> B[Select Source: Wiki / PDF / arXiv / Web]
    B --> C[Document Loader]
    C --> D[Text Splitter]
    D --> E[FAISS Vector DB + MiniLM Embeddings]
    E --> F[Relevant Chunks Retrieved]
    F --> G[Prompt Template Applied]
    G --> H[TinyLlama Local Model]
    H --> I[Answer Displayed in Streamlit UI]
```
</details>


## ğŸ“¸ Screenshot

Here is a screenshot of the output we get from multiple sources:

![Wikipedia](assets/wikipedia.png)

![PDF](assets/PDF.png)

![ArXiv](assets/ArXiv.png)

![Website](assets/Website.png)


## ğŸ§ª Example Use Cases

Gives context for when your chatbot might be helpful, such as:

a.Reading research papers (arXiv)

b.Classifying website data

c.Learning from Wikipedia

d.Extracting knowledge from PDFs

## ğŸ“Œ Notes

This clarifies:

a.Models are cached locally (TinyLlama + MiniLM)

b.Works offline

c.Only website/arXiv uses internet (for scraping)

## ğŸ“œ License
MIT License Â© 2025 [Parvathi Vishnu]


